---
layout: home
title: About
permalink: /
description: >-
    Course policies and information.
---

# About
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## About

The focus of this course is a graduate-level introduction to machine learning with a focus on supervised learning and pattern recognition (which we argue is the bedrock of most modern advances in the field). We try to present machine learning as a cohesive story where algorithms drop out of natural, specific assumptions about data. One might think of this story as how we, as a field, successively “generalized” our assumptions for how algorithmic techniques could process different types of data, culminating with our data being “internet-scale human language.” 

To do this, after the first couple foundational weeks, each week highlights a specific epistemic assumption about how our data is “generated,” and algorithms drop out of those assumptions. As a result, students should come away understanding not only which algorithms exist and are most useful in modern machine learning, but also the context and conditions that motivate each algorithm. Given the right assumptions, pattern recognition is possible; one way to view the story of machine learning is the evolution of such assumptions and how we’ve formalized these assumptions into algorithms.

A key “subplot” is the proliferation of data as historically driving machine learning progress and how it has obviated considerations of bias-variance “tradeoffs”. As we’ve hit upon more and more available data to train models, we could relax our inductive biases more and more. Another goal of this course is to make very clear how all these algorithms are positioned in this story in terms of their inductive bias vs. variance, and how the proliferation of large-scale data has naturally led us to our current, modern techniques.

## Lecture & Labs

- Lectures will be held on Tuesdays, from 2:45-4:45PM 
- Labs will be held on Thursdays, from 7:10-8PM

## Resources

The course does not have any official or required textbooks. However, we recommend the following optional resources:

- _Pattern Classification_, by Duda, Hardt, Stork
- _A Probabilistic Theory of Pattern Recognition_ by Devroye, Gyorfi, Lugosi
- [_Patterns, Predictions, and Actions_](https://mlstory.org) by Recht and Hardt
- _Understanding Machine Learning_ by Shalev-Shwartz and Ben-David
- _Boosting_ by Schapire and Freund
- [Daniel Hsu’s Machine Learning Course](https://www.cs.columbia.edu/~djhsu/coms4771-f25/)
- [Kyunghyun Cho’s Lecture Notes on Machine Learning](https://arxiv.org/pdf/2505.03861)
- Tom Mitchell’s [“Key Ideas in Machine Learning”](https://www.cs.cmu.edu/~tom/mlbook/keyIdeas.pdf)

## Assessment

Each week will feature a short homework assignment, intended to reinforce concepts from lecture. We will also have a midterm exam, as well as a final project. The grading breakdown is as follows:

- Homeworks: 20%
- Midterm Exam: 35%
- Final Project: 35%
- Lab Attendance: 10%

More details about the final project will be provided as the semester progresses!