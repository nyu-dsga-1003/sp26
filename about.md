---
layout: home
title: About
permalink: /
description: >-
    Course policies and information.
---

# About
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## About

The focus of this course is a graduate-level introduction to machine learning with a focus on supervised learning and pattern recognition (which we argue is the bedrock of most modern advances in the field). We try to present machine learning as a story where many algorithmic techniques drop out of a common statistical learning framework. 

The course covers a wide variety of topics in machine learning and statistical modeling. While mathematical methods and theoretical aspects will be covered, the primary goal is to provide students with the tools and principles needed to solve the data science problems found in practice. This course will serve as a foundation of knowledge on which more advanced courses and further independent study can build.

A key “subplot” of the course is the proliferation of data as historically driving machine learning progress and how it has obviated considerations of bias-variance “tradeoffs”. As we’ve hit upon more and more available data to train models, we could relax our inductive biases more and more. Another goal of this course is to make very clear how all these algorithms are positioned in this story in terms of their inductive bias vs. variance, and how the proliferation of large-scale data has naturally led us to our current, modern techniques.

## Prerequisites
This course requires some basic introductory knowledge of machine learning (ML) at the level of understanding the basic ML pipeline
from a "black-box" perspective. This should be familiar if students have taken DS-GA 1001 and DS-GA 1002; if there are concerns, please
email the instructors. An pre-recorded introductory "black-box machine learning" lecture will also be uploaded before the first week as 
a primer for students looking to refresh this knowledge before the course begins.

- **Solid mathematical background.** Equivalent to a 1-semester undergraduate course in: linear algebra, multivariable calculus, and statistics.
- **Programming background.** Ability to program in Python is required for most assignments.
- **DS-GA 1001: Introduction to Data Science** (or equivalent)
- **DS-GA-1002: Probability and Statistics for Data Science** (or equivalent)
- *Recommended, but not required:* At least one advanced, proof-based mathematics course.

## Lecture & Labs

- Lectures will be held on Tuesdays, from 2:45-4:45PM 
  - Location: 36 E 8th St (Cantor Film Ctr) Room 200
- Labs will be held on Thursdays, from 7:10-8PM

## Resources

The course does not have any official or required textbooks. However, we recommend the following optional resources:

- _Elements of Statistical Learning_ by Hastie, Tibshirani, and Friedman
- _Pattern Recognition and Machine Learning_ by Bishop
- _Pattern Classification_, by Duda, Hardt, Stork
- [_Patterns, Predictions, and Actions_](https://mlstory.org) by Recht and Hardt
- _Understanding Machine Learning_ by Shalev-Shwartz and Ben-David
- _Boosting_ by Schapire and Freund
- [Daniel Hsu’s Machine Learning Course](https://www.cs.columbia.edu/~djhsu/coms4771-f25/)
- [Kyunghyun Cho’s Lecture Notes on Machine Learning](https://arxiv.org/pdf/2505.03861)
- Tom Mitchell’s [“Key Ideas in Machine Learning”](https://www.cs.cmu.edu/~tom/mlbook/keyIdeas.pdf)

## Assessment

Each week will feature a short homework assignment, intended to reinforce concepts from lecture. We will also have a midterm exam, as well as a final project. The grading breakdown is as follows:

- Homeworks: 20%
- Midterm Exam: 35%
- Final Project: 35%
- Lab Attendance: 10%

More details about the final project will be provided as the semester progresses!